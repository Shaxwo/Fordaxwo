{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acefa922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shobika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65f42ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"status_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8107e836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(E,i):\n",
    "    # return 'True' if the point i exists \n",
    "    # in the array E, and 'False' unless otherwise\n",
    "    return 0 < np.shape(np.array( \\\n",
    "        [ e for e in np.array(E) if (e == i).all() ]))[0]\n",
    "\n",
    "def eucld(i1,i2):\n",
    "    # Compute the squared Euclidean distance d=|i1-i2|^2 as the sum of squared \n",
    "    # distances between points i1 and i2, at each dimension\n",
    "    return np.sum(np.array([ \\\n",
    "        math.pow(i1 - i2, 2.0) \\\n",
    "          for i1, i2 in zip(i1, i2) ]))\n",
    "\n",
    "def initialize(X):\n",
    "    # Get the random centroid c0\n",
    "    c0 = np.random.randint(0, np.shape(X)[0]) + 1\n",
    "\n",
    "    # Compute the distance from centroid c0 to each point in X\n",
    "    c0_d = np.array([ eucld(X[c0], x) for x in X ])\n",
    "    \n",
    "    # Get the centroid c1's as one of the points in X,\n",
    "    # having the maximum distance to the centroid c0\n",
    "    c1 = np.where(c0_d >= np.max(c0_d))[0][0]\n",
    "    \n",
    "    return np.array([c0,c1]) # Return the indexes of c0 and c1\n",
    "\n",
    "def compute(X,k):\n",
    "    X = np.array(X)    # X - an input dataset of n-observations\n",
    "    C = initialize(X)   # C - an initial set of centroids\n",
    "    \n",
    "    # Perform the dataset clustering iteratively, \n",
    "    # until the resultant set of k-clusters has been computed\n",
    "    \n",
    "    while True:\n",
    "        S = np.empty(0)  # S - a set of newly built clusters\n",
    "        \n",
    "        # For each observation x[t] in X, do the following:\n",
    "        for t in range(np.shape(X)[0]):\n",
    "            # Check if the observation x[t] has already been\n",
    "            # selected as one of the new centroids\n",
    "            if exists(C, t) == False:\n",
    "                # If not, compute the distance from \n",
    "                # the observation x[t] to each of the existing centroids in C\n",
    "                cn_ds = np.array([ eucld(X[t], X[c]) for c in C ])\n",
    "                # Get the centroid c[r] for which the distance to x[t] is the smallest\n",
    "                cn_min_di = np.where(cn_ds == np.min(cn_ds))[0][0]\n",
    "\n",
    "                # Assign the observation x[t] to the new cluster s[r], appending\n",
    "                # the observation x[t]'s and centroid c[r]'s indexes to the set S\n",
    "                S = np.append(S, { 'c': cn_min_di, 'i': t, 'd': cn_ds[cn_min_di] })\n",
    "\n",
    "        # Terminate the clustering process, if the number of centroids \n",
    "        # in C is equal to the total number of clusters k, initially specified.\n",
    "        \n",
    "        # Otherwise, compute the next centroid c[r] in C\n",
    "                \n",
    "        if np.shape(C)[0] >= k: break\n",
    "        \n",
    "        # Get the distances |x-c| from the observations \n",
    "        # accross all clusters in S to each of the centroids in C\n",
    "        cn_ds = np.array([s['d'] for s in S ])\n",
    "        \n",
    "        # Compute the index of an observation, for which \n",
    "        # the distance to one of the centroids in C is the largest\n",
    "        cn_max_ci = np.where(cn_ds == np.max(cn_ds))[0][0]\n",
    "\n",
    "        # Append the index of a new centroid c[r] to the set C\n",
    "        C = np.append(C, S[cn_max_ci]['i'])\n",
    "\n",
    "    return C,S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "67f4e0fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_samples=1 should be >= n_clusters=5.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#converting toarrray() to get a dense matrix\u001b[39;00m\n\u001b[0;32m      5\u001b[0m X \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(df)\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m----> 6\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m \u001b[43mKMeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m kmeans\u001b[38;5;241m.\u001b[39mpredict(X\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1146\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;124;03m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[0;32m   1113\u001b[0m \n\u001b[0;32m   1114\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1138\u001b[0m     X,\n\u001b[0;32m   1139\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1143\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1144\u001b[0m )\n\u001b[1;32m-> 1146\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[0;32m   1148\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:947\u001b[0m, in \u001b[0;36mKMeans._check_params\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;66;03m# n_clusters\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters:\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should be >= n_clusters=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    949\u001b[0m     )\n\u001b[0;32m    951\u001b[0m \u001b[38;5;66;03m# tol\u001b[39;00m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tol \u001b[38;5;241m=\u001b[39m _tolerance(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol)\n",
      "\u001b[1;31mValueError\u001b[0m: n_samples=1 should be >= n_clusters=5."
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import kmeans_plusplus\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "#converting toarrray() to get a dense matrix\n",
    "X = vectorizer.fit_transform(df).toarray()\n",
    "kmeans = KMeans(n_clusters=5).fit(X.reshape(-1,1))\n",
    "kmeans.predict(X.reshape(-1,1))\n",
    "# X=X.reshape(1,-1)\n",
    "# centers, indices = kmeans_plusplus(X, n_clusters=3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a77b787b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clusters = compute(V_s,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2252e7c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_samples=1 should be >= n_clusters=9.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [64]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m K:\n\u001b[1;32m----> 9\u001b[0m     km \u001b[38;5;241m=\u001b[39m \u001b[43mkm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     km \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, n_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     12\u001b[0m     Sum_of_squared_distances\u001b[38;5;241m.\u001b[39mappend(km\u001b[38;5;241m.\u001b[39minertia_)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1146\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;124;03m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[0;32m   1113\u001b[0m \n\u001b[0;32m   1114\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1138\u001b[0m     X,\n\u001b[0;32m   1139\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1143\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1144\u001b[0m )\n\u001b[1;32m-> 1146\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[0;32m   1148\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:947\u001b[0m, in \u001b[0;36mKMeans._check_params\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;66;03m# n_clusters\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters:\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should be >= n_clusters=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    949\u001b[0m     )\n\u001b[0;32m    951\u001b[0m \u001b[38;5;66;03m# tol\u001b[39;00m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tol \u001b[38;5;241m=\u001b[39m _tolerance(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol)\n",
      "\u001b[1;31mValueError\u001b[0m: n_samples=1 should be >= n_clusters=9."
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "Sum_of_squared_distances = []\n",
    "K = range(2,10)\n",
    "for k in K:\n",
    "\n",
    "    km = km.fit(X.reshape(-1,1))\n",
    "    km = KMeans(n_clusters=9, max_iter=200, n_init=10)\n",
    "  \n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()\n",
    "print('How many clusters do you want to use?')\n",
    "true_k = int(input())\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=200, n_init=10)\n",
    "\n",
    "model.fit(X)\n",
    "\n",
    "labels=model.labels_\n",
    "clusters=pd.DataFrame(list(zip(text,labels)),columns=['title','cluster'])\n",
    "    #print(clusters.sort_values(by=['cluster']))\n",
    "\n",
    "for i in range(true_k):\n",
    "    print(clusters[clusters['cluster'] == i])\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc8b45b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster=cluster_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8feef56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def idf_transform(word_col):\n",
    "#     #words present in how many documents - df\n",
    "#     w = len(word_col[np.nonzero(word_col)])\n",
    "#     #compute idf for a word\n",
    "#     return np.log(len(word_col)/(w + 1))\n",
    "\n",
    "# def tf_idf(bow):\n",
    "#     #TF matrix\n",
    "#     tf = np.log(bow + 1)\n",
    "#     #1d array\n",
    "#     idf = np.apply_along_axis(idf_transform,0,bow)\n",
    "#     #tf-idf\n",
    "#     return (np.multiply(tf,idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7466ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V_s_tfidf = tf_idf(V_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2b542891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# # initialize kmeans with 3 centroids\n",
    "# kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "# # fit the model\n",
    "# kmeans.fit(V_s_tfidf.reshape(-1,1))\n",
    "# # store cluster labels in a variable\n",
    "# clusters = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "356c9386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,len(clusters)):\n",
    "#     print(amz.iloc[clusters[i]][['status_text']].sample(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7bfe91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
