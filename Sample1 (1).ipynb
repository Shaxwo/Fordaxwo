{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acefa922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shobika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65f42ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>On cost for PIA ringing to be agreed - If high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>Per Hugh Campbell and Colin Horbal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>Issue Type changed from \"Design\" to \"Meets Des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>Nyogel, OD improvements, and pending changes t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125</th>\n",
       "      <td>[CFT, 6/19/15] DENSO Report #1, \"DI Injector P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            status_text\n",
       "630   On cost for PIA ringing to be agreed - If high...\n",
       "549                  Per Hugh Campbell and Colin Horbal\n",
       "1009  Issue Type changed from \"Design\" to \"Meets Des...\n",
       "311   Nyogel, OD improvements, and pending changes t...\n",
       "1125  [CFT, 6/19/15] DENSO Report #1, \"DI Injector P..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amz=pd.read_csv(\"status_text.csv\")\n",
    "amz.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2404e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93d2c719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amz.status_text.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "517abf87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amz.status_text[(amz.status_text.str.len() == 0) | (amz.status_text == \"\")].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "878a14f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of dataset before dropping the NAs: (1211, 1)\n",
      "Dimensions of dataset after dropping the NAs: (1211, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensions of dataset before dropping the NAs:\",amz.shape)\n",
    "amz = amz.dropna(subset=['status_text'])\n",
    "print(\"Dimensions of dataset after dropping the NAs:\",amz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b850a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to  :  1407\n",
      "the  :  894\n",
      "from  :  517\n",
      "and  :  465\n",
      "for  :  432\n",
      "issue  :  392\n",
      "of  :  373\n",
      "no  :  366\n",
      "with  :  359\n",
      "on  :  324\n",
      "be  :  308\n",
      "is  :  301\n",
      "in  :  273\n",
      "value  :  232\n",
      "will  :  230\n",
      "a  :  222\n",
      "by  :  220\n",
      "this  :  218\n",
      "that  :  169\n",
      "as  :  158\n",
      "pca  :  158\n",
      "at  :  153\n",
      "not  :  151\n",
      "date  :  135\n",
      "pd  :  125\n",
      "ica  :  125\n",
      "l  :  124\n",
      "was  :  119\n",
      "m  :  118\n",
      "have  :  110\n",
      "parts  :  108\n",
      "are  :  102\n",
      "has  :  100\n",
      "we  :  97\n",
      "d  :  93\n",
      "per  :  83\n",
      "new  :  81\n",
      "me  :  81\n",
      "aims  :  80\n",
      "part  :  76\n",
      "p  :  73\n",
      "been  :  73\n",
      "test  :  71\n",
      "see  :  70\n",
      "team  :  70\n",
      "build  :  69\n",
      "s  :  67\n",
      "ford  :  65\n",
      "up  :  63\n",
      "need  :  61\n",
      "can  :  61\n",
      "if  :  61\n",
      "gm  :  61\n",
      "close  :  61\n",
      "added  :  58\n",
      "data  :  58\n",
      "all  :  57\n",
      "pfi  :  54\n",
      "model  :  53\n",
      "oil  :  53\n",
      "an  :  51\n",
      "type  :  50\n",
      "it  :  49\n",
      "aim  :  49\n",
      "due  :  47\n",
      "after  :  47\n",
      "head  :  47\n",
      "hole  :  44\n",
      "may  :  43\n",
      "next  :  43\n",
      "were  :  42\n",
      "lead  :  42\n",
      "still  :  42\n",
      "plan  :  41\n",
      "there  :  40\n",
      "cam  :  40\n",
      "e  :  40\n",
      "mm  :  40\n",
      "out  :  39\n",
      "c  :  39\n",
      "vp  :  38\n",
      "year  :  37\n",
      "nano  :  37\n",
      "when  :  37\n",
      "i  :  37\n",
      "cause  :  36\n",
      "g  :  36\n",
      "or  :  35\n",
      "plant  :  35\n",
      "seal  :  34\n",
      "being  :  34\n",
      "cost  :  34\n",
      "f  :  34\n",
      "fm  :  34\n",
      "leak  :  34\n",
      "ring  :  34\n",
      "final  :  33\n",
      "block  :  33\n",
      "b  :  33\n",
      "tool  :  33\n",
      "print  :  32\n",
      "study  :  32\n",
      "th  :  32\n",
      "week  :  32\n",
      "they  :  32\n",
      "apr  :  32\n",
      "based  :  31\n",
      "ptme  :  31\n",
      "crank  :  31\n",
      "cover  :  30\n",
      "xm  :  30\n",
      "now  :  29\n",
      "front  :  29\n",
      "lep  :  29\n",
      "meet  :  28\n",
      "level  :  28\n",
      "so  :  28\n",
      "t  :  28\n",
      "re  :  27\n",
      "mark  :  27\n",
      "would  :  27\n",
      "but  :  27\n",
      "time  :  26\n",
      "any  :  26\n",
      "gear  :  26\n",
      "open  :  26\n",
      "gtdi  :  26\n",
      "run  :  26\n",
      "fwd  :  25\n",
      "do  :  25\n",
      "below  :  24\n",
      "work  :  24\n",
      "r  :  24\n",
      "check  :  24\n",
      "also  :  23\n",
      "pt  :  23\n",
      "add  :  23\n",
      "tt  :  23\n",
      "first  :  23\n",
      "aug  :  23\n",
      "heat  :  23\n",
      "ht  :  23\n",
      "same  :  22\n",
      "x  :  22\n",
      "meets  :  22\n",
      "line  :  21\n",
      "into  :  21\n",
      "move  :  21\n",
      "v  :  21\n",
      "other  :  21\n",
      "which  :  21\n",
      "spec  :  21\n",
      "only  :  21\n",
      "both  :  21\n",
      "ltp  :  21\n",
      "rwd  :  21\n",
      "root  :  20\n",
      "bore  :  20\n",
      "order  :  20\n",
      "case  :  20\n",
      "shaft  :  20\n",
      "valve  :  20\n",
      "ok  :  20\n",
      "fox  :  20\n",
      "pump  :  20\n",
      "drive  :  20\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "d=dict()\n",
    "for i in range(1,len(amz)):\n",
    "    status_text = re.sub('[^a-zA-Z]', ' ', amz['status_text'][i])\n",
    "    status_text = status_text.lower()\n",
    "    status_text = status_text.split()\n",
    "    corpus.append(status_text)\n",
    "    for word in status_text:\n",
    "        if (len(word)<6):\n",
    "            if word in d:\n",
    "                d[word] = d[word] + 1\n",
    "            else:\n",
    "                d[word] = 1\n",
    "    ps = PorterStemmer()\n",
    "    status_text = [ps.stem(word) for word in status_text\n",
    "                if not word in set(stopwords.words('english'))]\n",
    "    status_text = ' '.join(status_text)\n",
    "dope = dict((k, v) for k, v in d.items() if v >= 20)\n",
    "\n",
    "marklist = sorted(dope.items(), key=lambda x:x[1], reverse=True)\n",
    "sortdict = dict(marklist)\n",
    "for key,val in sortdict.items(): \n",
    "    print(key, ' : ', val)\n",
    "# res = {key: val for key, val in sorted(d.items() , key = lambda ele: ele[1], reverse = True) if val>20}\n",
    "# for key in list(d.keys()):\n",
    "#         print(key, \":\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "077a8837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<WordListCorpusReader in '.../corpora/stopwords' (not loaded yet)>\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4fad8fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ek', 'awaiting', 'lab', 'results', 'micrographs'],\n",
       " ['lab', 'results', 'returned', 'and', 'reviewing', 'on', 'jan'],\n",
       " ['results',\n",
       "  'from',\n",
       "  'lab',\n",
       "  'show',\n",
       "  'that',\n",
       "  'no',\n",
       "  'contamination',\n",
       "  'or',\n",
       "  'degraded',\n",
       "  'materials',\n",
       "  'were',\n",
       "  'found',\n",
       "  'during',\n",
       "  'analysis',\n",
       "  'and',\n",
       "  'that',\n",
       "  'the',\n",
       "  'returned',\n",
       "  'parts',\n",
       "  'meet',\n",
       "  'the',\n",
       "  'current',\n",
       "  'specs',\n",
       "  'stresses',\n",
       "  'in',\n",
       "  'the',\n",
       "  'side',\n",
       "  'walls',\n",
       "  'in',\n",
       "  'the',\n",
       "  'crack',\n",
       "  'area',\n",
       "  'were',\n",
       "  'also',\n",
       "  'very',\n",
       "  'low',\n",
       "  'therefore',\n",
       "  'the',\n",
       "  'crack',\n",
       "  'did',\n",
       "  'not',\n",
       "  'come',\n",
       "  'from',\n",
       "  'application',\n",
       "  'loading',\n",
       "  'on',\n",
       "  'the',\n",
       "  'cover',\n",
       "  'additional',\n",
       "  'tests',\n",
       "  'are',\n",
       "  'ongoing',\n",
       "  'to',\n",
       "  'determine',\n",
       "  'root',\n",
       "  'cause',\n",
       "  'see',\n",
       "  'new',\n",
       "  'attachment'],\n",
       " ['ek',\n",
       "  'running',\n",
       "  'additional',\n",
       "  'fea',\n",
       "  'modeling',\n",
       "  'looking',\n",
       "  'at',\n",
       "  'out',\n",
       "  'of',\n",
       "  'flat',\n",
       "  'skewing',\n",
       "  'effects',\n",
       "  'that',\n",
       "  'could',\n",
       "  'generate',\n",
       "  'extra',\n",
       "  'stresses',\n",
       "  'causing',\n",
       "  'the',\n",
       "  'cracks',\n",
       "  'discussed',\n",
       "  'the',\n",
       "  'tardiness',\n",
       "  'of',\n",
       "  'closing',\n",
       "  'pending',\n",
       "  'issue',\n",
       "  'and',\n",
       "  'hope',\n",
       "  'to',\n",
       "  'be',\n",
       "  'more',\n",
       "  'expedient',\n",
       "  'in',\n",
       "  'the',\n",
       "  'future'],\n",
       " ['the',\n",
       "  'results',\n",
       "  'presented',\n",
       "  'by',\n",
       "  'ek',\n",
       "  'show',\n",
       "  'that',\n",
       "  'with',\n",
       "  'a',\n",
       "  'low',\n",
       "  'level',\n",
       "  'of',\n",
       "  'warpage',\n",
       "  'out',\n",
       "  'of',\n",
       "  'flatness',\n",
       "  'the',\n",
       "  'stresses',\n",
       "  'near',\n",
       "  'the',\n",
       "  'crack',\n",
       "  'area',\n",
       "  'increase',\n",
       "  'will',\n",
       "  'be',\n",
       "  're',\n",
       "  'running',\n",
       "  'fea',\n",
       "  'with',\n",
       "  'a',\n",
       "  'higher',\n",
       "  'level',\n",
       "  'of',\n",
       "  'warpage',\n",
       "  'see',\n",
       "  'attachment'],\n",
       " ['attached',\n",
       "  'd',\n",
       "  'need',\n",
       "  'to',\n",
       "  'get',\n",
       "  'additional',\n",
       "  'out',\n",
       "  'of',\n",
       "  'flatness',\n",
       "  'test',\n",
       "  'results',\n",
       "  'as',\n",
       "  'well',\n",
       "  'as',\n",
       "  'the',\n",
       "  'implementation',\n",
       "  'date',\n",
       "  'for',\n",
       "  'corrective',\n",
       "  'action',\n",
       "  'out',\n",
       "  'of',\n",
       "  'flatness',\n",
       "  'end',\n",
       "  'of',\n",
       "  'line',\n",
       "  'testing',\n",
       "  'on',\n",
       "  'production',\n",
       "  'processes'],\n",
       " ['attached',\n",
       "  'updated',\n",
       "  'd',\n",
       "  'pca',\n",
       "  'implemented',\n",
       "  'molded',\n",
       "  'covers',\n",
       "  'from',\n",
       "  'production',\n",
       "  'tools',\n",
       "  'have',\n",
       "  'windage',\n",
       "  'cut',\n",
       "  'into',\n",
       "  'them',\n",
       "  'to',\n",
       "  'meet',\n",
       "  'part',\n",
       "  'flatness',\n",
       "  'requirements',\n",
       "  'prototype',\n",
       "  'parts',\n",
       "  'were',\n",
       "  'used',\n",
       "  'on',\n",
       "  'build',\n",
       "  'additional',\n",
       "  'fea',\n",
       "  'analysis',\n",
       "  'showed',\n",
       "  'cracks',\n",
       "  'can',\n",
       "  'result',\n",
       "  'in',\n",
       "  'same',\n",
       "  'area',\n",
       "  'due',\n",
       "  'to',\n",
       "  'out',\n",
       "  'of',\n",
       "  'flatness',\n",
       "  'and',\n",
       "  'twisting',\n",
       "  'loads',\n",
       "  'during',\n",
       "  'bolt',\n",
       "  'down',\n",
       "  'recommend',\n",
       "  'to',\n",
       "  'move',\n",
       "  'to',\n",
       "  'pending'],\n",
       " ['added',\n",
       "  'final',\n",
       "  'supplier',\n",
       "  'd',\n",
       "  'issue',\n",
       "  'should',\n",
       "  'be',\n",
       "  'closed',\n",
       "  'to',\n",
       "  'root',\n",
       "  'cause',\n",
       "  'of',\n",
       "  'free',\n",
       "  'state',\n",
       "  'flatness',\n",
       "  'out',\n",
       "  'of',\n",
       "  'specification',\n",
       "  'due',\n",
       "  'to',\n",
       "  'prototype',\n",
       "  'molded',\n",
       "  'part',\n",
       "  'not',\n",
       "  'to',\n",
       "  'print'],\n",
       " ['closed',\n",
       "  'to',\n",
       "  'supplier',\n",
       "  'final',\n",
       "  'd',\n",
       "  'issue',\n",
       "  'was',\n",
       "  'found',\n",
       "  'on',\n",
       "  'prototype',\n",
       "  'parts',\n",
       "  'and',\n",
       "  'will',\n",
       "  'be',\n",
       "  'corrected',\n",
       "  'for',\n",
       "  'production',\n",
       "  'parts'],\n",
       " ['lead',\n",
       "  'vehicle',\n",
       "  'model',\n",
       "  'year',\n",
       "  'changed',\n",
       "  'from',\n",
       "  'nano',\n",
       "  'l',\n",
       "  'v',\n",
       "  'fwd',\n",
       "  'cd',\n",
       "  'to',\n",
       "  'nano',\n",
       "  'l',\n",
       "  'l',\n",
       "  'v',\n",
       "  'upgrade',\n",
       "  'p']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a397222",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# fit_transform applies TF-IDF to clean texts - we save the array of vectors in X\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2077\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2058\u001b[0m \u001b[38;5;124;03m\"\"\"Learn vocabulary and idf, return document-term matrix.\u001b[39;00m\n\u001b[0;32m   2059\u001b[0m \n\u001b[0;32m   2060\u001b[0m \u001b[38;5;124;03mThis is equivalent to fit followed by transform, but more efficiently\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   2075\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2076\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m-> 2077\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2078\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2079\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2080\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1330\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1322\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1323\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1324\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1325\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1326\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1327\u001b[0m             )\n\u001b[0;32m   1328\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1330\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1333\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1201\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1200\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1201\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1202\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1203\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:71\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 71\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.95)\n",
    "# fit_transform applies TF-IDF to clean texts - we save the array of vectors in X\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67f4e0fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# fit_transform applies TF-IDF to clean texts - we save the array of vectors in X\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2077\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2058\u001b[0m \u001b[38;5;124;03m\"\"\"Learn vocabulary and idf, return document-term matrix.\u001b[39;00m\n\u001b[0;32m   2059\u001b[0m \n\u001b[0;32m   2060\u001b[0m \u001b[38;5;124;03mThis is equivalent to fit followed by transform, but more efficiently\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   2075\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2076\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m-> 2077\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2078\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2079\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2080\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1330\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1322\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1323\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1324\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1325\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1326\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1327\u001b[0m             )\n\u001b[0;32m   1328\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1330\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1333\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1201\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1200\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1201\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1202\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1203\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:71\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 71\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# vectorizer = CountVectorizer(stop_words='english')\n",
    "# #converting toarrray() to get a dense matrix\n",
    "# V_s = vectorizer.fit_transform(corpus).toarray()\n",
    "vectorizer = TfidfVectorizer()\n",
    "# fit_transform applies TF-IDF to clean texts - we save the array of vectors in X\n",
    "X = vectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea1c9972",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 24.7 GiB for an array with shape (310103, 10677) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# fit the model\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mkmeans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# store cluster labels in a variable\u001b[39;00m\n\u001b[0;32m      5\u001b[0m clusters \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1137\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1112\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[0;32m   1113\u001b[0m \n\u001b[0;32m   1114\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1137\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params(X)\n\u001b[0;32m   1147\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:566\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 566\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    567\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:821\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    815\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m feature(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m a minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_features, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_features, context)\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmay_share_memory(array, array_orig):\n\u001b[1;32m--> 821\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 24.7 GiB for an array with shape (310103, 10677) and data type float64"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "# fit the model\n",
    "kmeans.fit(X)\n",
    "# store cluster labels in a variable\n",
    "clusters = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feef56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf_transform(word_col):\n",
    "    #words present in how many documents - df\n",
    "    w = len(word_col[np.nonzero(word_col)])\n",
    "    #compute idf for a word\n",
    "    return np.log(len(word_col)/(w + 1))\n",
    "\n",
    "def tf_idf(bow):\n",
    "    #TF matrix\n",
    "    tf = np.log(bow + 1)\n",
    "    #1d array\n",
    "    idf = np.apply_along_axis(idf_transform,0,bow)\n",
    "    #tf-idf\n",
    "    return (np.multiply(tf,idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7466ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf_idf(V_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7cb95d24",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 24.7 GiB for an array with shape (310103, 10677) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeans\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# kmeans = KMeans(n_clusters=6)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m km \u001b[38;5;241m=\u001b[39m \u001b[43mKMeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m km\u001b[38;5;241m.\u001b[39mlabels_\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1137\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1112\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[0;32m   1113\u001b[0m \n\u001b[0;32m   1114\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1137\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params(X)\n\u001b[0;32m   1147\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:566\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 566\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    567\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:821\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    815\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m feature(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m a minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_features, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_features, context)\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmay_share_memory(array, array_orig):\n\u001b[1;32m--> 821\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 24.7 GiB for an array with shape (310103, 10677) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# kmeans = KMeans(n_clusters=6)\n",
    "km = KMeans(n_clusters=6).fit(X)\n",
    "km.labels_.tolist()\n",
    "# Y = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0ca2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,len(amz)):\n",
    "#      print(amz,km.labels_.tolist())\n",
    "res = \"\\n\".join(\"{} {}\".format(x, y) for x, y in zip(amz['status_text'], km.labels_.tolist()))\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc86a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_top_keywords(n_terms):\n",
    "#     \"\"\"This function returns the keywords for each centroid of the KMeans\"\"\"\n",
    "#     df = pd.DataFrame(X.todense()).groupby(clusters).mean() # groups the TF-IDF vector by cluster\n",
    "#     terms = vectorizer.get_feature_names_out() # access tf-idf terms\n",
    "#     for i,r in df.iterrows():\n",
    "#         print('\\nCluster {}'.format(i))\n",
    "#         print(','.join([terms[t] for t in np.argsort(r)[-n_terms:]])) # for each row of the dataframe, find the n terms that have the highest tf idf score\n",
    "            \n",
    "# get_top_keywords(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b542891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def kmeans(V,k):\n",
    "#     #select k reviews as centers\n",
    "#     k_center_i = random.sample(range(0,V.shape[0]),k)\n",
    "#     center_v = V[k_center_i, :]\n",
    "    \n",
    "#     #all reviews\n",
    "#     A_i = np.array([x for x in range(0,V.shape[0])])\n",
    "#     all_v_norm = np.apply_along_axis(np.linalg.norm,1,V)\n",
    "    \n",
    "#     #clusters - initial\n",
    "#     clusters = [None] * k\n",
    "#     clusters[0] = A_i.tolist()\n",
    "#     for i in range(1,k):\n",
    "#         clusters[i] = []\n",
    "#     j=0\n",
    "#     while True:\n",
    "        \n",
    "#         print(j)\n",
    "#         for i in range(0,len(clusters)):\n",
    "#             print('cluster',i,len(clusters[i]))\n",
    "# #         #only printing the sizes of first 4 clusters\n",
    "# #         print(\"iteration\",j,\"cluster0\",len(clusters[0]),\"cluster1\",len(clusters[1]),\"cluster2\",len(clusters[2]),\n",
    "# #               \"clusters3\",len(clusters[3]))\n",
    "#         #Norm of cluster center vectors\n",
    "#         center_v_norm = np.apply_along_axis(np.linalg.norm,1,center_v)\n",
    "#         #Cosine similarity: \n",
    "#         #x @ y\n",
    "#         product_v = V @ np.transpose(center_v)\n",
    "#         #divide by norms ||x|| and ||y||\n",
    "#         product_v_n = np.apply_along_axis(np.true_divide,1,product_v,center_v_norm)\n",
    "#         product_v_norm = np.apply_along_axis(np.true_divide,0,product_v_n,all_v_norm)\n",
    "#         #get each review has maximum cosine similarity with which center\n",
    "#         max_center = np.argmax(product_v,axis=1)\n",
    "\n",
    "#         #assign to closest clusters\n",
    "#         clusters_new = [None] * k\n",
    "#         for i in range(k):\n",
    "#             r = np.where(np.array(max_center) == i)\n",
    "#             clusters_new[i] = r[0].tolist()\n",
    "\n",
    "#         if (np.array_equal(clusters,clusters_new)):\n",
    "#             break\n",
    "#         else:\n",
    "#             j = j+1\n",
    "        \n",
    "#         #calculate new centers\n",
    "#         for i in range(k):\n",
    "#             reviews = V[clusters_new[i], :]\n",
    "#             center_v[i] = np.mean(reviews,axis=0)\n",
    "        \n",
    "#         #set old clusters as new clusters\n",
    "#         clusters = clusters_new.copy()\n",
    "    \n",
    "#     print(\"Clusters converged after\",j+1,\"iterations\")\n",
    "#     return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a77b787b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clusters = kmeans(X,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "356c9386",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(amz)):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(amz\u001b[38;5;241m.\u001b[39miloc[\u001b[43mclusters\u001b[49m[i]][[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus_text\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m7\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clusters' is not defined"
     ]
    }
   ],
   "source": [
    "# for i in range(0,len(clusters)):\n",
    "#     print(amz.iloc[clusters[i]][['status_text']].sample(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bd4fda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
